{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable\n",
    "os.environ['VANTAGE_TOKEN'] = ''\n",
    "os.environ['CLICKHOUSE_HOST'] = ''\n",
    "os.environ['CLICKHOUSE_USER'] = ''\n",
    "os.environ['CLICKHOUSE_PASSWORD'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = clickhouse_connect.get_client(\n",
    "    host=os.getenv(\"CLICKHOUSE_HOST\"),\n",
    "    user=os.getenv(\"CLICKHOUSE_USER\"),\n",
    "    password=os.getenv(\"CLICKHOUSE_PASSWORD\"),\n",
    "    secure=True\n",
    ")\n",
    "\n",
    "SCALE_CONFIGS = [\n",
    "    'https://raw.githubusercontent.com/pytorch/test-infra/main/.github/scale-config.yml',\n",
    "    'https://raw.githubusercontent.com/pytorch/test-infra/main/.github/lf-scale-config.yml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_runner_type(runner_type):\n",
    "    runner_type = re.sub(r'am2\\.', '', runner_type)\n",
    "    runner_type = re.sub(r'amz2\\.', '', runner_type)\n",
    "    runner_type = re.sub(r'amz2023\\.', '', runner_type)\n",
    "    runner_type = re.sub(r'c\\.', '', runner_type)\n",
    "    runner_type = re.sub(r'.canary$', '', runner_type)\n",
    "\n",
    "    return runner_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use clickhouse to get all entries in the last DAYS days for the table workflow_job\n",
    "\n",
    "days_back = 14\n",
    "start_time = time.time()\n",
    "\n",
    "start_date = pd.Timestamp.today().date() - pd.Timedelta(days=days_back)\n",
    "logger.info(f\"Will fetch data from the last {days_back} days\")\n",
    "\n",
    "COLUMNS = ['started_at', 'name', 'labels', 'dynamoKey', 'completed_at',\n",
    "            'conclusion', 'created_at', 'steps', 'head_branch', 'workflow_name']\n",
    "\n",
    "# max of MAX_DAYS days after start_date\n",
    "end_date = pd.Timestamp.today().date()\n",
    "logger.info(f\"Getting data from {start_date} to {end_date} (inclusive)\")\n",
    "query_start_time = time.time()\n",
    "query = f\"select {','.join(COLUMNS)} from workflow_job where started_at >= DATE('{start_date}') and started_at <= DATE('{end_date}')\"\n",
    "workflow_jobs = client.query(query).result_set\n",
    "logger.info(f\"Query took {time.time() - query_start_time:.2f} seconds\")\n",
    "\n",
    "workflow_jobs_df = pd.DataFrame(workflow_jobs, columns=COLUMNS)\n",
    "logger.info(f\"Found {workflow_jobs_df.shape[0]} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove everything that's not finished yet (completed_at is still 1970)\n",
    "workflow_jobs_df = workflow_jobs_df[workflow_jobs_df['completed_at'].dt.year > 2000]\n",
    "\n",
    "# remove the item 'self-hosted' from the list of runner types\n",
    "workflow_jobs_df['labels'] = workflow_jobs_df['labels'].map(\n",
    "    lambda l: [x for x in l if x != 'self-hosted'])\n",
    "\n",
    "workflow_jobs_df['runner_type'] = workflow_jobs_df['labels'].map(lambda l: l[0] if len(l) > 0 else None)\n",
    "\n",
    "# drop rows where labels is empty\n",
    "workflow_jobs_df = workflow_jobs_df[workflow_jobs_df['labels'].map(\n",
    "    len) > 0]\n",
    "\n",
    "workflow_jobs_df['group_repo'] = workflow_jobs_df['dynamoKey'].map(\n",
    "    lambda s: '/'.join(s.split('/')[:2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for runner_type in workflow_jobs_df['runner_type'].unique():\n",
    "    out[runner_type] = {}\n",
    "    for d in range(days_back+1):\n",
    "        # hour is 0 padded\n",
    "        for hour in range(24):\n",
    "            for minute in range(0, 60):\n",
    "                out[runner_type][f\"{start_date + pd.Timedelta(days=d)}_{hour:02d}-{minute:02d}\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we loop over the dataframe and add the cost to the right key. The cost needs to be added to all hours where the job was running, so if a job ran from 1:20 to 2:40, it should add +1 to the 1-2h and +1 to the 2-3h key\n",
    "print(\"TOTAL:    \"+\".\"*100)\n",
    "print(\"PROGRESS: \", end=\"\")\n",
    "tot=0\n",
    "for index, row in workflow_jobs_df.iterrows():\n",
    "    if index % (len(workflow_jobs_df)//100) == 99:\n",
    "        print(\".\", end=\"\")\n",
    "    start = row['started_at']\n",
    "    end = row['completed_at']\n",
    "    runner_type = row['runner_type']\n",
    "    \n",
    "    cur_dt = start.floor('min')\n",
    "    \n",
    "    while cur_dt < end:\n",
    "        cur_dt_str = cur_dt.strftime('%Y-%m-%d_%H-%M')\n",
    "        out[runner_type][cur_dt_str] += 1\n",
    "        cur_dt += pd.Timedelta(minutes=1)\n",
    "        tot += 1\n",
    "        \n",
    "print(\"\\nFINISHED\")\n",
    "print(f\"Total: {tot}\")\n",
    "       \n",
    "# now make a df from the out dict\n",
    "out_df = pd.DataFrame(out).T\n",
    "out_df.columns = pd.to_datetime(out_df.columns, format='%Y-%m-%d_%H-%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# only the top 10 lines\n",
    "# show x-axis as continuous date, not as every timestamp\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "plt.title(f\"# runners per type in the last {days_back} days\")\n",
    "plt.ylabel(\"Parallel runners\")\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# take the top 10, only the ones that have nvidia in the name\n",
    "out_df_nvidia = out_df[out_df.index.str.contains('nvidia', case=False)]\n",
    "\n",
    "for runner_type in out_df_nvidia.T.sum().sort_values(ascending=False).index[:10]:\n",
    "    sns.lineplot(data=out_df_nvidia.loc[runner_type], label=runner_type)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df_summary = out_df.copy()\n",
    "out_df_summary['average'] = out_df.mean(axis=1).round(0).astype(int)\n",
    "out_df_summary['p90'] = out_df.quantile(0.9, axis=1).round(0).astype(int)\n",
    "out_df_summary['p99'] = out_df.quantile(0.99, axis=1).round(0).astype(int)\n",
    "\n",
    "out_df_summary = out_df_summary.sort_values('average', ascending=False)\n",
    "\n",
    "out_df_summary = out_df_summary[out_df_summary.index.str.contains('nvidia', case=False)]\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(out_df_summary[['average', 'p90', 'p99']])\n",
    "pd.reset_option('display.max_rows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
