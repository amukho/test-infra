# Week of 2025-04-21 to 2025-04-28 (33)

### GHFirst (7)

- [Revert "Rewrite the guts of torch::jit::Lexer to speed it up (#151850)"](https://github.com/pytorch/pytorch/commit/fa1b4ef6499cacf82da13b5e4da857e0e2e24f66)
  - This codev PR is breaking  on it's internal counterpart diff D73129443.  For codev PRs like this one, please always make sure the internal diff is green and then land the diff internally. The Github PR will be automatically merged ([comment](https://github.com/pytorch/pytorch/pull/151850#issuecomment-2831686141))
- [Revert "[Inductor] Record Tritonâ€™s Base32 Cache Key in .best_config for Debugging (#148981)"](https://github.com/pytorch/pytorch/commit/c03359de2dc3d0794c6857d87c0a73b98dd7d43d)
  - Sorry but this is breaking internally. @davidberard98 can you please help get these changes validated? Details in D73628297. To validate the fixes internally, you can follow the instructions here: https://fburl.com/fixing-ghfirst-reverts ([comment](https://github.com/pytorch/pytorch/pull/148981#issuecomment-2831044810))
- [Revert "[Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event (#151404)"](https://github.com/pytorch/pytorch/commit/67f75244ea4e2b82b5252092f1a0dd848213df41)
  - Sorry but this is breaking internally. @albanD can you please help it get relanded? To validate the fixes internally, you can follow the instructions here: https://fburl.com/fixing-ghfirst-reverts ([comment](https://github.com/pytorch/pytorch/pull/151404#issuecomment-2830829368))
- [Revert "[Cutlass] Implement EVT example tensor creation (#150904)"](https://github.com/pytorch/pytorch/commit/3a170a8ce6878f386576c37f59ea0e5cb94d5c88)
  - Sorry but this is breaking the test_example_tensor_creation test internally. See D73519195 for more details. To validate your fixes internally, you can follow the instructions here: https://fburl.com/fixing-ghfirst-reverts ([comment](https://github.com/pytorch/pytorch/pull/150904#issuecomment-2828132914))
- [Revert "[cutlass backend] delay construction of cutlass presets to when called (#151875)"](https://github.com/pytorch/pytorch/commit/aa285e6512bae860981ffeb2547b85946fde0027)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/151875#issuecomment-2825030726))
- [Revert "[Inductor] Add Additional Configs for persistent+TMA version of Triton mm and addmm (#150587)"](https://github.com/pytorch/pytorch/commit/3804aed32e09ad5c633ac2378dbe9eb0d7555cc7)
  - Sorry but this is breaking internally (see D73410693). To validate your fixes internally, you can follow the instructions here: https://fburl.com/fixing-ghfirst-reverts ([comment](https://github.com/pytorch/pytorch/pull/150587#issuecomment-2821828926))
- [Revert "[ez] Make relaxed constraint error message more user friendly (#151407)"](https://github.com/pytorch/pytorch/commit/4504910843a29cd4d560b1a4161452db5d3d1c9e)
  - Sorry but this is breaking internally (see D73198095). To validate your fixes internally, you can follow the instructions here: https://fburl.com/fixing-ghfirst-reverts. ([comment](https://github.com/pytorch/pytorch/pull/151407#issuecomment-2821819654))

### Ignored Signal (3)

- [Revert "[MPS] Adjust test_sum_dtypes so it can run on MPS. (#152064)"](https://github.com/pytorch/pytorch/commit/43f1b60dedfd56afe5266d90afa88063610afce2)
  - Lint is not green ([comment](https://github.com/pytorch/pytorch/pull/152064#issuecomment-2826305781))
- [Revert "Update torch-xpu-ops commit pin (#150827)"](https://github.com/pytorch/pytorch/commit/8172397025471033f69c5b5b65e0f71d02bedd69)
  - Inductor UT regression ([comment](https://github.com/pytorch/pytorch/pull/150827#issuecomment-2825857903))
- [Revert "[inductor] Change minimum number of SMs to 60 to let Ada use Triton GEMM backend (#150888)"](https://github.com/pytorch/pytorch/commit/72f711e200858ee5b1b1f37d1d351e05d04e28fd)
  - Revert because this change isn't needed ([comment](https://github.com/pytorch/pytorch/pull/150888#issuecomment-2822768377))

### Not through pytorchbot (1)

- [Back out "Do not propagate real tensor in extern kernel" (#151813)](https://github.com/pytorch/pytorch/commit/efdcc981d054d194b72643f20086b134cee06433)

### No Signal (12)

- [Revert "Update OpenBLAS commit  (#151547)"](https://github.com/pytorch/pytorch/commit/c02edba86388d1f86a78bce99d16c5405b54086e)
  - It breaks all aarch64 tests ([comment](https://github.com/pytorch/pytorch/pull/151547#issuecomment-2833593427))
- [Revert "[BE] Do not allow PyTorch codebase to use `c10::optional` (#150464)"](https://github.com/pytorch/pytorch/commit/0f765773e30923126a061feaf087607b70910ef0)
  - broke xpu [GH job link](https://github.com/pytorch/pytorch/actions/runs/14674243034/job/41187443432) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/490ef768cff448080083a46f362053e025f6b95b)? ([comment](https://github.com/pytorch/pytorch/pull/150464#issuecomment-2831608162))
- [Revert "[dynamo] Add guard serialization for tensor matches. (#151318)"](https://github.com/pytorch/pytorch/commit/b1d055fd6a023565527aa754d5a76cd3c575a996)
  - macos test failing ([comment](https://github.com/pytorch/pytorch/pull/151318#issuecomment-2828638168))
- [Revert "Turn on static cuda launcher in OSS (#151691)"](https://github.com/pytorch/pytorch/commit/562328501e167206dc7d4b16895b5ae538520e06)
  - This breaks tests, see https://hud.pytorch.org/hud/pytorch/pytorch/c1f51cf2c4fc8259fa48bc506320118e0e907906/1?per_page=50&name_filter=linux-focal-cuda12.6-py3.10&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/151691#issuecomment-2825427252))
- [Revert "[MPS] Fix test_neg_index_mps (#151966)"](https://github.com/pytorch/pytorch/commit/98c53d8b39734504e5a76df51eb0533885cf1ed4)
  - Looks like it broke halide testing, see https://github.com/pytorch/pytorch/actions/runs/14623941238/job/41034065229 ([comment](https://github.com/pytorch/pytorch/pull/151966#issuecomment-2825425305))
- [Revert "[FlexAttention] Fix device test instantation (#151846)"](https://github.com/pytorch/pytorch/commit/7310049c425413e9bd2a009fe5029fe45d43d2ff)
  - PR broke rocm workflow ([comment](https://github.com/pytorch/pytorch/pull/151846#issuecomment-2824607429))
- [Revert "faster gather implementation (#151490)"](https://github.com/pytorch/pytorch/commit/f072bf27a75e090cac72a10579dcf98bfdc9e2df)
  - Looks like it breaks demucs accuracy, though may be bogus, but let's try to revert, see https://hud.pytorch.org/hud/pytorch/pytorch/c729f7dbee3be20f04a6aeac41ae0cd5be23403d/3?per_page=50&name_filter=inductor_torchbench%2C%201%2C%202&mergeEphemeralLF=true ([comment](https://github.com/pytorch/pytorch/pull/151490#issuecomment-2821803788))
- [Revert "Non-deterministic alert in histc_cuda for floating types only (#151701)"](https://github.com/pytorch/pytorch/commit/ed0d2ebaa09479f5615da8e3b8703f60f72ae9b9)
  - Sorry but this is causing inductor tests to fail. See here for more info: test_torch.py::TestTorchDeviceTypeCUDA::test_nondeterministic_alert_histc_cuda_float32 [GH job link](https://github.com/pytorch/pytorch/actions/runs/14586002763/job/40913547718) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/b7a7741411585817daa81780b078fd15816f2d2d) ([comment](https://github.com/pytorch/pytorch/pull/151701#issuecomment-2821800837))
- [Revert "[dynamic shapes] guard_or_false for _reshape_view_helper, utils._infer_size for wildcard dims (#150127)"](https://github.com/pytorch/pytorch/commit/e76c0b159a8a2becd342119c0438bc1d19e3f22a)
  - Caused TestDynamoTimed.test_dynamo_timed to fail on macOS, see https://github.com/pytorch/pytorch/actions/runs/14584536979/job/40908019050 ([comment](https://github.com/pytorch/pytorch/pull/150127#issuecomment-2820081721))
- [Revert "reroute index to fast implementation for indexing on 0th dimension (#151753)"](https://github.com/pytorch/pytorch/commit/0ff302e8e059cdfda64be6733ded301bcd18d4e2)
  - Looks like it breaks bunch of distributed tests with DSA, see https://hud.pytorch.org/pytorch/pytorch/commit/4d78e19365c4e2189693c7a81b665d4ec2d2cf53 ([comment](https://github.com/pytorch/pytorch/pull/151753#issuecomment-2820078298))
- [Revert "[Intel GPU] Allow XPU backend in Depthwise_conv2d&3d operators (#149114)"](https://github.com/pytorch/pytorch/commit/40cf49d4607cf59453193321986eac34a8fbaa93)
  - CI is broken, 'aten::_conv_depthwise2d' is not currently implemented ([comment](https://github.com/pytorch/pytorch/pull/149114#issuecomment-2819890341))
- [Revert "[Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event (#151404)"](https://github.com/pytorch/pytorch/commit/9374064483eb988ac7fb44d359b165f2cf86fabc)
  - suspected of breaking linux builds and breaks internal tests as well ([comment](https://github.com/pytorch/pytorch/pull/151404#issuecomment-2819041756))

### Weird (10)

- [Revert "Add OIDC permissions to bazel workflow (#151456)"](https://github.com/pytorch/pytorch/commit/8313bc27f2e1625a16622cb1d88be40c163e4959)
  - This is causing downstream failures on PRs, see examples in PR comment ([comment](https://github.com/pytorch/pytorch/pull/151456#issuecomment-2829130319))
- [Revert "[fake tensor cache] Support index with non bool/int8 indices (#151477)"](https://github.com/pytorch/pytorch/commit/9344da8bd10f079018a0ebec4d384a10176dab0a)
  - reverting confusing ghstack state ([comment](https://github.com/pytorch/pytorch/pull/151477#issuecomment-2825023953))
- [Revert "[invoke_subgraph][fake tensor] Add finalizer on subgraph instead of the functionalize ctx wrapper (#151633)"](https://github.com/pytorch/pytorch/commit/348272e67e64fc97f2a2586b2380df85ea0d74f3)
  - reverting confusing ghstack state ([comment](https://github.com/pytorch/pytorch/pull/151633#issuecomment-2825007363))
- [Revert "[Optimus][Observability] Improve tlparse logging (#151635)"](https://github.com/pytorch/pytorch/commit/835413baed0293baa38b97b6f26c5965eaa665ca)
  - broke dynamo/test_structured_trace.py::StructuredTraceTest::test_ddp_graphs [GH job link](https://github.com/pytorch/pytorch/actions/runs/14600342064/job/40970324075) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/06a3c3c8cdb2424d42d7926a49a18ee6852a40cb), test did fail on PR but dr ci says it matches an existing failure, which it does, but also this PR breaks the test too ([comment](https://github.com/pytorch/pytorch/pull/151635#issuecomment-2822538113))
- [Revert "Do not generate long log messaged for suppressed data dependent errors. (#151023)"](https://github.com/pytorch/pytorch/commit/bc6c0bc344ed87351f088a1a178f89b88ac14b1e)
  - breaking other PRs ([comment](https://github.com/pytorch/pytorch/pull/151023#issuecomment-2822483635))
- [Revert "Do not log exception when recording is disabled or already recording (#151038)"](https://github.com/pytorch/pytorch/commit/459c62ee1dbf84611da77140a7beee131524f737)
  - breaking other PRs ([comment](https://github.com/pytorch/pytorch/pull/151023#issuecomment-2822483635))
- [Revert "Log information about suppressed data dependent errors (#151041)"](https://github.com/pytorch/pytorch/commit/aaf71a481b67c587faa0497da975024f02d9468b)
  - breaking other PRs ([comment](https://github.com/pytorch/pytorch/pull/151023#issuecomment-2822483635))
- [Revert "[compile][compile time traces] Add more dynamo traces (#151357)"](https://github.com/pytorch/pytorch/commit/0bb9b89fb736d8496ccc86271f34a69a38d4ac02)
  - stack in a weird state - reverting for now ([comment](https://github.com/pytorch/pytorch/pull/151357#issuecomment-2822369232))
- [Revert "[aot autograd][logging] Profile large missing gaps in compile time tracing (#151256)"](https://github.com/pytorch/pytorch/commit/fd04c79878ee1f0d47405d880d8896c6187de369)
  - breaking internal tests, cannot import ([comment](https://github.com/pytorch/pytorch/pull/151256#issuecomment-2819244186))
- [Revert "[Easy] The event_id of torch.cuda.Event and torch.xpu.Event always is 0 (#151226)"](https://github.com/pytorch/pytorch/commit/33808f0ebdc9a13348ecce75d860a2e5cf58bc6f)
  - Reverting to unblock revert of https://github.com/pytorch/pytorch/pull/151404 ([comment](https://github.com/pytorch/pytorch/pull/151226#issuecomment-2819030735))
