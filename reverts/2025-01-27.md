# Week of 2025-01-27 to 2025-02-03 (22)

### GHFirst (12)

- [Revert "[inductor/profiler] add kernel kwargs instrumentation (#145573)"](https://github.com/pytorch/pytorch/commit/16f44fee258c352d854e4ef2c4ef43e9b8175cec)
  - Sorry, but this is failing internally. It's a bit weird since this PR doesn't really appear related at first glance, but despite retries it fails pretty consistently. Please see D68930742 for details ([comment](https://github.com/pytorch/pytorch/pull/145573#issuecomment-2628013872))
- [Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)"](https://github.com/pytorch/pytorch/commit/c3f71eb61b474f750fd79ca29409dddb6365504e)
  - Sorry but this still fails internally with the same error.  @Chillee or @malfet, can you please help the change get tested? (See D68783351) ([comment](https://github.com/pytorch/pytorch/pull/144441#issuecomment-2627886999))
- [Revert "inductor: Don't throw an internal error when a nn.module is missing a attribute (#145122)"](https://github.com/pytorch/pytorch/commit/f5a61ba0a3dd2aa0954718af990febab04b9b979)
  - Sorry but this is failing internally. See D68924977 for details ([comment](https://github.com/pytorch/pytorch/pull/145122#issuecomment-2627880860))
- [Revert "[c10d] Add NCCL memory allocator (#145675)"](https://github.com/pytorch/pytorch/commit/5fa28bbe40759c7d5a8517d6f4d5a615f26a774f)
  - Sorry but this still fails internally. See D68866823 for details ([comment](https://github.com/pytorch/pytorch/pull/145675#issuecomment-2624900562))
- [Revert "[c10d] Add NCCL memory allocator (#145675)"](https://github.com/pytorch/pytorch/commit/6371c25b91fb66756b5a2df67741d4e7a72a7261)
  - This fails to build internally, can you please take a look at D68831004 for more details? ([comment](https://github.com/pytorch/pytorch/pull/145675#issuecomment-2622515425))
- [Revert "inductor.config.descriptive_names = False is not actually supported (#145523)"](https://github.com/pytorch/pytorch/commit/e0525dbca9c5cec43c62a7a330de4a4590079178)
  - Hi, this breaks type checks internally. Can you please take a look? See D68801083 for details ([comment](https://github.com/pytorch/pytorch/pull/145523#issuecomment-2622510900))
- [Revert "[Environment Variable][7/N] Use thread-safe getenv functions (#140211)"](https://github.com/pytorch/pytorch/commit/284f217011eebdf128aa5b1671e5e98ef9fb94ab)
  - Sorry but this is failing internally. @eqy @ezyang can you please help this get remerged? See D68779772. ([comment](https://github.com/pytorch/pytorch/pull/140211#issuecomment-2622504898))
- [Revert "Record inputs at time of tracing, constrain to them for triton fn (#145448)"](https://github.com/pytorch/pytorch/commit/0d6343347fb122004753c45ec1b95b25833e9120)
  - Sorry but this is breaking internally. See D68779678 for details ([comment](https://github.com/pytorch/pytorch/pull/145448#issuecomment-2622470810))
- [Revert "[inductor][BE] Enable test_cpu_cpp_wrapper in fbcode (#145373)"](https://github.com/pytorch/pytorch/commit/cfbb27462eb7a734b980e283e2289f5d4996c91d)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/145373#issuecomment-2619674197))
- [Revert "Remove lexicographical sorting of storage keys in torch.save (#143879)"](https://github.com/pytorch/pytorch/commit/dbef2a9bc93a464cfb18eebde6b1d2094381d2da)
  - Sorry but this is breaking internally. See D68746524 for details ([comment](https://github.com/pytorch/pytorch/pull/143879#issuecomment-2619661492))
- [Revert "pickler for GraphModule (#141659)"](https://github.com/pytorch/pytorch/commit/2de53b3b6592d9bdadbf979be47d2f8d51b7d821)
  - Sorry but this is breaking internally, please take a look at D68694181 for more details. ([comment](https://github.com/pytorch/pytorch/pull/141659#issuecomment-2617045120))
- [Revert "[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)"](https://github.com/pytorch/pytorch/commit/c986eba560ba920b06371f4bcb333c9031d74983)
  - Sorry but this is failing internally. @Chillee can you please help change get remerged? See  D68720562 ([comment](https://github.com/pytorch/pytorch/pull/144441#issuecomment-2616726406))

### No Signal (7)

- [Revert "Tensor .cuda() very slow with specific array sizes (#138964)"](https://github.com/pytorch/pytorch/commit/c39c67981310cbad3d422b5ae0316686f31de78b)
  - Sorry for reverting your PR but some slow test start failing after this lands ([comment](https://github.com/pytorch/pytorch/pull/138964#issuecomment-2628455198))
- [Revert "Advance past fc window for stft center (#145437)"](https://github.com/pytorch/pytorch/commit/4280232f2145a83a1971bf51cd58e6c3247e28bc)
  - Sorry for reverting your change but it breaks some slow trunk tests ([comment](https://github.com/pytorch/pytorch/pull/145437#issuecomment-2625840742))
- [Revert "Update mi300 labels to account for multiple clusters. (#145923)"](https://github.com/pytorch/pytorch/commit/967cf85f3a872494bafddeded5eadf750eb56bed)
  - reverting back to one cluster ([comment](https://github.com/pytorch/pytorch/pull/145923#issuecomment-2625022826))
- [Revert "[ATen][CUDA] Implement 128 bit vectorization v2 (#145746)"](https://github.com/pytorch/pytorch/commit/b60120d0df9654fd38300d6f67b0d7ddda024945)
  - Sorry but this is breaking in trunk. See functorch/test_ops.py::TestOperatorsCUDA::test_jvp_nn_functional_multi_head_attention_forward_cuda_float32 [GH job link](https://github.com/pytorch/pytorch/actions/runs/13032483748/job/36358184032) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/81685d81eb86595d169f55a564da26eaafb2ddf5) ([comment](https://github.com/pytorch/pytorch/pull/145746#issuecomment-2623108958))
- [Revert "[dynamo] Use polyfill to implement comparison operators (#144485)"](https://github.com/pytorch/pytorch/commit/1185b81c5140b338ee8f2866027a181bcc69b3a5)
  - This seems to break dynamo tests in trunk after landing ([comment](https://github.com/pytorch/pytorch/pull/144485#issuecomment-2622893294))
- [Revert "[dynamo] save/restore system random state more carefully (#145750)"](https://github.com/pytorch/pytorch/commit/3481c2aec4caaf512ed2d1ecb49730eaae465150)
  - bisected perf regression ([comment](https://github.com/pytorch/pytorch/pull/145750#issuecomment-2620028414))
- [Revert "Add option to serialization config to reduce random reads from get_record_offset when loading with mmap=True (#143880)"](https://github.com/pytorch/pytorch/commit/9010649292880fc2df6d9a94adf351f742d23dc4)
  - Sorry for reverting your change, but either this PR or the base PR breaks distributed tests ([comment](https://github.com/pytorch/pytorch/pull/143880#issuecomment-2617743403))

### Weird (3)

- [Revert "[triton] Update pin to tip of 3.2 release (#145867)"](https://github.com/pytorch/pytorch/commit/7391cea857bba10f02ff3f5efc46d530c4c3241b)
  - Sorry, this PR may have been written correctly, but something is clearly broken with the infra that's making CI very unhappy with this new triton version.  Since this has been blocking viable/strict upgrades for a couple days now, I'm reverting this PR.  I'll sync with @atalman on how we should fix this. ([comment](https://github.com/pytorch/pytorch/pull/145867#issuecomment-2625720817))
- [Revert "[CMake] Find HomeBrew OpenMP on MacOS (#145870)"](https://github.com/pytorch/pytorch/commit/b80482988fa72d059fc947478e53dbb374be1551)
  - Want to refine it a bit ([comment](https://github.com/pytorch/pytorch/pull/145870#issuecomment-2622659614))
- [Revert "[CD] Install ninja and setuptools from PyPI (#145871)"](https://github.com/pytorch/pytorch/commit/b52e8d521e279f70d106a42b9354f4d33022f663)
  - Want to refine it a bit ([comment](https://github.com/pytorch/pytorch/pull/145870#issuecomment-2622659614))
