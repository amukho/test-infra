# Week of 2024-10-07 to 2024-10-14 (33)

### GHFirst (5)

- [Revert "Make Context to be Device-agnostic Step by Step (1/N) (#136519)"](https://github.com/pytorch/pytorch/commit/079f909263fb15fa11f9af8e988abf6244effe4c)
  - this pr is causing errors internally ([comment](https://github.com/pytorch/pytorch/pull/136519#issuecomment-2405781093))
- [Revert "Make Context to be Device-agnostic Step by Step (2/N) (#136526)"](https://github.com/pytorch/pytorch/commit/33e5921e6b284c77d7e78b2d09fce9adef9a6499)
  - this pr is causing errors internally ([comment](https://github.com/pytorch/pytorch/pull/136519#issuecomment-2405781093))
- [Revert "Log chromium event for automatic dynamic reasons (#137491)"](https://github.com/pytorch/pytorch/commit/c73d2634b9164d4c7085cd2c420d963347efca74)
  - breaking internal tests ([comment](https://github.com/pytorch/pytorch/pull/137491#issuecomment-2403360486))
- [Revert "Disallow FakeTensor.data_ptr access in eager mode (#137221)"](https://github.com/pytorch/pytorch/commit/796c3c34152b7dc10d846c8c02fae1d7947275f5)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/137221#issuecomment-2397957081))
- [Revert "Add back DistributedDataParallel types that were lost when pyi was removed (#136835)"](https://github.com/pytorch/pytorch/commit/fe44b6a67f32b562c88701b630e65b62ce1b63ba)
  - this pr is causing typecheck errors internally ([comment](https://github.com/pytorch/pytorch/pull/136835#issuecomment-2397661940))

### Ignored Signal (3)

- [Revert "BundledAutotuneCache (#134959)"](https://github.com/pytorch/pytorch/commit/1358969fa1a0a5ebf9452b4538e11add9f84c68b)
  - The newly added test fails on rocm CI ([comment](https://github.com/pytorch/pytorch/pull/134959#issuecomment-2408091754))
- [Revert "In Inductor, be willing to generate deferred runtime asserts when unbacked (#137097)"](https://github.com/pytorch/pytorch/commit/f69bf005f7b3be70b9a38ba663e6bc825dfe7ffd)
  - Sorry for reverting your change, it seems to increase the compilation time a lot causing some jobs to timeout ([comment](https://github.com/pytorch/pytorch/pull/137097#issuecomment-2404573266))
- [Revert "[AutoAC] Backward Pass Aware AC - changes to partitioner to acommodate SOLVER as a callable (#137314)"](https://github.com/pytorch/pytorch/commit/2fff990c1628a09f1a01c18b60d0cbe1b2b44285)
  - The failure shows up in trunk ([comment](https://github.com/pytorch/pytorch/pull/137314#issuecomment-2401311719))

### Landrace (1)

- [Revert "Port Inductor dataclasses to be kw_only (#137768)"](https://github.com/pytorch/pytorch/commit/41977a05314bbf537e1c5d6cf5916a368d1907d9)
  - Sorry for reverting your change, but it seem to fail test_loop_ordering in trunk ([comment](https://github.com/pytorch/pytorch/pull/137768#issuecomment-2409203115))

### No Signal (12)

- [Revert "Add device agnostic API for accelerator hooks (#137480)"](https://github.com/pytorch/pytorch/commit/563e9f99c3de8a24fc740927dc12a0eec7895d8b)
  - break all builds on trunk ([comment](https://github.com/pytorch/pytorch/pull/137480#issuecomment-2408954802))
- [Revert "Add support for add in tensorify_python_scalars fx pass (#137620)"](https://github.com/pytorch/pytorch/commit/70bd58c35f434af9acf5a8cba4585341a43f66e5)
  - Sorry for reverting your change, but it seems to cause test_torchbind_inductor to fail in trunk https://hud.pytorch.org/pytorch/pytorch/commit/0430e72e755d2c1953917ffb78f00c516eb4bbd5 ([comment](https://github.com/pytorch/pytorch/pull/137620#issuecomment-2408784170))
- [Revert "Add support for sub in tensorify_python_scalars fx pass (#137622)"](https://github.com/pytorch/pytorch/commit/279052ab867c8a03779446d52a0230c973e31ab6)
  - Sorry for reverting your change, but it seems to cause test_torchbind_inductor to fail in trunk https://hud.pytorch.org/pytorch/pytorch/commit/0430e72e755d2c1953917ffb78f00c516eb4bbd5 ([comment](https://github.com/pytorch/pytorch/pull/137620#issuecomment-2408784170))
- [Revert "[AOTI] Handle inplace output in ProxyExecutor (#137660)"](https://github.com/pytorch/pytorch/commit/0121d64aa9edf55d7e93d11a75fbf4f0a5ed2976)
  - Fails in fbcode ([comment](https://github.com/pytorch/pytorch/pull/137660#issuecomment-2408213485))
- [Revert "[AOTI] Turn on the ABI-compatible mode as default (#136534)"](https://github.com/pytorch/pytorch/commit/c58e5c4efa740f69b0a5f8568ffebb0486d8bc47)
  - The dependent PR https://github.com/pytorch/pytorch/pull/137660 fails in fbcode ([comment](https://github.com/pytorch/pytorch/pull/136534#issuecomment-2408211238))
- [Revert "Upgrade distributed test to g4dn instances (T4 GPUs) (#137161)"](https://github.com/pytorch/pytorch/commit/4fb1fd8a516cec04bbdd6c8f9f5b07682d21ab8e)
  - broken tests on trunk ([comment](https://github.com/pytorch/pytorch/pull/137161#issuecomment-2406236337))
- [Revert "[Distributed] Fix extra context on device 0 (#135273)"](https://github.com/pytorch/pytorch/commit/b55ff476bde2bbfc9366f79312a9c06d3ea34f96)
  - broken tests on trunk ([comment](https://github.com/pytorch/pytorch/pull/137161#issuecomment-2406236337))
- [Revert "Introduce torch.sym_sum (#136429)"](https://github.com/pytorch/pytorch/commit/16a2c2cfd4ac58eecb05accbdf6b845a1ffbd9cf)
  - fails internal stuff ([comment](https://github.com/pytorch/pytorch/pull/136429#issuecomment-2403335147))
- [Revert "[FSDP2] Required `mesh_dim_names` for HSDP (#137436)"](https://github.com/pytorch/pytorch/commit/5e3e1c01515bac92bbe303d3fdbfee571949a96f)
  - Looks like it broke distributed testing, see https://github.com/pytorch/pytorch/actions/runs/11239761070/job/31249854217 ([comment](https://github.com/pytorch/pytorch/pull/137436#issuecomment-2400794929))
- [Revert "[ROCm] remove caffe2 from hipify (#137157)"](https://github.com/pytorch/pytorch/commit/7e8dace0de6bb589e4fd8f37e8642819b80c0baa)
  - this is breaking internal where we still use caffe2 ([comment](https://github.com/pytorch/pytorch/pull/137157#issuecomment-2400466131))
- [Revert "Add support for cat memory planning mms with max autotune (#132554)"](https://github.com/pytorch/pytorch/commit/493d0eeef3978dacfcb89a04bb3cf395e62f344f)
  - Sorry for reverting your change but I think it is failing on ROCm ([comment](https://github.com/pytorch/pytorch/pull/132554#issuecomment-2398946854))
- [Revert "[BE][Ez]: Update cudnn_frontend submodule to v1.7.0 (#136920)"](https://github.com/pytorch/pytorch/commit/01c07e786423e67d41e5a1cc55915511cb7fabac)
  - Breaks sdpa with bias support, will switch to newer patch version when released ([comment](https://github.com/pytorch/pytorch/pull/136920#issuecomment-2397548622))

### Weird (12)

- [Revert "[Dynamo] Trace enter/exit of TorchFunctionModes (#135422) (#137114)"](https://github.com/pytorch/pytorch/commit/d34b617bb98cc18a42e0af4df8e275f40c25dca9)
  - The top of the stack has been reverted but it leaves trunk in a broken state, so I try to revert the rest of the stack ([comment](https://github.com/pytorch/pytorch/pull/137114#issuecomment-2400765603))
- [Revert "[Dynamo] Remove ignored modes workaround (#135502) (#137115)"](https://github.com/pytorch/pytorch/commit/8c937445ee1e05c43566873c01c2de8bb19145d3)
  - The top of the stack has been reverted but it leaves trunk in a broken state, so I try to revert the rest of the stack ([comment](https://github.com/pytorch/pytorch/pull/137114#issuecomment-2400765603))
- [Revert "[Dynamo] Remove ignored modes from torch function mode stack guard (#135503) (#137116)"](https://github.com/pytorch/pytorch/commit/e5f91313278c11b8bae45331e809eae573a82588)
  - The top of the stack has been reverted but it leaves trunk in a broken state, so I try to revert the rest of the stack ([comment](https://github.com/pytorch/pytorch/pull/137114#issuecomment-2400765603))
- [Revert "[Dynamo] Ensure torch function modes are dispatched on builtin ops (#137117)"](https://github.com/pytorch/pytorch/commit/2d18c2d5e76b77db6c7b28763b285d81c2a6c7f9)
  - The top of the stack has been reverted but it leaves trunk in a broken state, so I try to revert the rest of the stack ([comment](https://github.com/pytorch/pytorch/pull/137114#issuecomment-2400765603))
- [Revert "Parametrize test_lstm_packed (#137447)"](https://github.com/pytorch/pytorch/commit/5349ee2934c0a8ef21f8ba335e23f7f211ae0c4a)
  - Need to up few more instance to 4xlarge, revert to reland ([comment](https://github.com/pytorch/pytorch/pull/137447#issuecomment-2400737602))
- [Revert "[FlexAttention] Support training bias for eager (#136910)"](https://github.com/pytorch/pytorch/commit/a8047564ff481fb9cfb2b2c41d61d5408553e827)
  - torch.library.custom_op looks weird here and it breaks some internal workloads ([comment](https://github.com/pytorch/pytorch/pull/136910#issuecomment-2400434833))
- [Revert "[Dynamo] Move flex attention torch function mode to traceable HOP file (#137120)"](https://github.com/pytorch/pytorch/commit/0b5ade8a12719b8f94ec3bd908d8047f4343cd19)
  - Need to revert to be able to revert https://github.com/pytorch/pytorch/pull/136910 ([comment](https://github.com/pytorch/pytorch/pull/137120#issuecomment-2400429265))
- [Revert "type _dynamo/trace_wrapped_higher_order_op.py (#137354)"](https://github.com/pytorch/pytorch/commit/2570d77a265cf527ef541f16e3a9d6362a032d5d)
  - Need to revert to be able to revert https://github.com/pytorch/pytorch/pull/136910 ([comment](https://github.com/pytorch/pytorch/pull/137354#issuecomment-2400424669))
- [Revert "[Dynamo] Handle extracted unbound tensor methods (#137227)"](https://github.com/pytorch/pytorch/commit/76c5bdd2cc476a32e3f93950579f2de9230c9998)
  - Need to revert to be able to revert https://github.com/pytorch/pytorch/pull/136910 ([comment](https://github.com/pytorch/pytorch/pull/137227#issuecomment-2400406384))
- [Revert "[Dynamo] Handle torch function subclass/mode dispatch on generic tensor methods (#137119)"](https://github.com/pytorch/pytorch/commit/c88c0e6c659b356f545bbd079886eb7831a41df2)
  - Need to revert to be able to revert https://github.com/pytorch/pytorch/pull/136910 ([comment](https://github.com/pytorch/pytorch/pull/137119#issuecomment-2400401262))
- [Revert "[Dynamo] add flex attention mode test (#137121)"](https://github.com/pytorch/pytorch/commit/cc10ef464569060483d776740f47db70a4123082)
  - Need to revert to be able to revert https://github.com/pytorch/pytorch/pull/136910 ([comment](https://github.com/pytorch/pytorch/pull/137121#issuecomment-2400389882))
- [Revert "[FlexAttention] only calculate grads for buffers that require_grad (#137451)"](https://github.com/pytorch/pytorch/commit/11192ceca4c66ee0ba752eed3b06091a12e0733c)
  - Need to revert it in order to be able to backout https://github.com/pytorch/pytorch/pull/136910 ([comment](https://github.com/pytorch/pytorch/pull/137451#issuecomment-2400385858))
