# Week of 2024-09-23 to 2024-09-30 (24)

### GHFirst (11)

- [Revert "Fix numerical instability for norm (#129352)"](https://github.com/pytorch/pytorch/commit/f21b471978266489f1b243660f73e7cd0252bb22)
  - Breaks Internal CI ([comment](https://github.com/pytorch/pytorch/pull/129352#issuecomment-2379989485))
- [Revert "Add Triton CPU as an Inductor backend (#133408)"](https://github.com/pytorch/pytorch/commit/36428f91e969dd27c6f2aa2665da3e5180ba4deb)
  - internal tests failing ([comment](https://github.com/pytorch/pytorch/pull/133408#issuecomment-2379692517))
- [Revert "Add deterministic path for CUDA `cumsum` (#136224)"](https://github.com/pytorch/pytorch/commit/e9d2765ec8f04be38867add5774c031ad1a0798d)
  - Break internal CI ([comment](https://github.com/pytorch/pytorch/pull/136224#issuecomment-2379214226))
- [Revert "[inductor] Triton codegen: Use scalar when creating f64 constant instead of 1-element tensor (#136594)"](https://github.com/pytorch/pytorch/commit/5eb68d565fab476f901939a25f774d5b71f40386)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/136594#issuecomment-2378358302))
- [Revert "Introduce _ArglessActivation base class for parameterless activation functions (#136296)"](https://github.com/pytorch/pytorch/commit/d5e4a20c171441459e96cbbd0f99bdfe0d8f0e88)
  - Breaks Internal CI. Error: Too many arguments [19]: Call `nn.modules.activation._ArglessActivation.__init__` expects 0 positional arguments, 1 was provided. ([comment](https://github.com/pytorch/pytorch/pull/136296#issuecomment-2377091280))
- [Revert "Fix constant propagation in builtins and UserClasses (#131354)"](https://github.com/pytorch/pytorch/commit/9223c16208bdb7055523d855a79cbce5d186bc5b)
  - Breaks torchrec tests ([comment](https://github.com/pytorch/pytorch/pull/131354#issuecomment-2375417145))
- [Revert "Add deterministic path for CUDA `cumsum` (#136224)"](https://github.com/pytorch/pytorch/commit/e3b89ca1242796666c522115dfb937d9ec528297)
  - Failing internall CI ([comment](https://github.com/pytorch/pytorch/pull/136224#issuecomment-2374201626))
- [Revert "[RFC][torchelastic][c10d] Fix store prefix race in rendezvous (#135957)"](https://github.com/pytorch/pytorch/commit/706eda5cd860fd654a250e107ad93facf66c5557)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/135957#issuecomment-2372493186))
- [Revert "Add deterministic path for CUDA `cumsum` (#136224)"](https://github.com/pytorch/pytorch/commit/fd182b90a74918d60d598383dcdc4af7c6dc73b7)
  - Failing internall CI ([comment](https://github.com/pytorch/pytorch/pull/136224#issuecomment-2369244135))
- [Revert "[AOTI] Create another wrapper class to handle ArrayRef (#136318)"](https://github.com/pytorch/pytorch/commit/274883083d99511d33a6b75af40c99f7613fe40d)
  - Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/136318#issuecomment-2368957264))
- [Revert "[aotd] Fix freezing API for subclasses (#136265)"](https://github.com/pytorch/pytorch/commit/df6a8fa1eb9f989f5bf59415ded0f7caa0b8ed49)
  - Breaks internal CI sorry, need to revert ([comment](https://github.com/pytorch/pytorch/pull/136265#issuecomment-2368772574))

### Not through pytorchbot (2)

- [Revert "Update run_test.py"](https://github.com/pytorch/pytorch/commit/a619ced5ed9d034b756850a07732f15c36e60a98)
- [Revert "[Dynamo] Trace enter/exit of TorchFunctionModes (#135422)" (#136590)](https://github.com/pytorch/pytorch/commit/289df45cee05edb0872b0df0f8a93c8d1dd5b5ca)

### No Signal (9)

- [Revert "[Flex Attention] fix block size order (#136657)"](https://github.com/pytorch/pytorch/commit/f7ab0e99893cb06d933c22de3e55720b1ac2176b)
  - Sorry, this seems to break ROCm builds. inductor/test_flex_attention.py::TestFlexAttention::test_builtin_score_mods_seqlen_lt_custom_sparse_block_size_float16_score_mod1 [GH job link](https://github.com/pytorch/pytorch/actions/runs/11069782242/job/30759299713) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/b42f1e3641314c8dc369255b850450acddf3477c) ([comment](https://github.com/pytorch/pytorch/pull/136657#issuecomment-2380031525))
- [Revert "[user triton] Make tl.constexpr specialization work for triton_op & capture_triton (#136686)"](https://github.com/pytorch/pytorch/commit/287dc3639553731ab150592fbb31f95dbb5c593e)
  - breaks lint on main. Please rebase to see and fix the error ([comment](https://github.com/pytorch/pytorch/pull/136686#issuecomment-2379830921))
- [Revert "Deal with size oblivious before going into worker (#135137)"](https://github.com/pytorch/pytorch/commit/de159f0c8d4511041fb2ff9d75fa5d45192bfca1)
  - this is the one that actually broke main ([comment](https://github.com/pytorch/pytorch/pull/135137#issuecomment-2379438566))
- [Revert "Don't uselessly recompute axiom dict every static eval call (#135429)"](https://github.com/pytorch/pytorch/commit/e5228a77712138291521e08bd521e6ae9ad5ba2c)
  - It(or it's parent) broke trunk CI, see https://hud.pytorch.org/pytorch/pytorch/commit/507c69e20f645fdb0fbf43b05be0c5117971464e ([comment](https://github.com/pytorch/pytorch/pull/135429#issuecomment-2379422971))
- [Revert "Correctly convert Python float to float64 when passing argument as Tensor (#136413)"](https://github.com/pytorch/pytorch/commit/0133fbcfe71963be5288e7d848e39817f2ca07f4)
  - forward fix is stuck, revert this ([comment](https://github.com/pytorch/pytorch/pull/136413#issuecomment-2372404873))
- [Revert "Increase update_hint_regression problem size to 1000 (#136434)"](https://github.com/pytorch/pytorch/commit/064093a4d6766a7e55e7a9006c5fe39c2189399f)
  - whoops, this is too slow ([comment](https://github.com/pytorch/pytorch/pull/136434#issuecomment-2371847842))
- [Revert "Fix tensor.data_ptr() representation overflow (#135567)"](https://github.com/pytorch/pytorch/commit/538ee7bf600fcda19b7d720f6c91864c475c0673)
  - Block XPU, let's re-land with triton update. ([comment](https://github.com/pytorch/pytorch/pull/135567#issuecomment-2371200549))
- [Revert "Adds support for accelerated sorting with x86-simd-sort (#127936)"](https://github.com/pytorch/pytorch/commit/0e19522122b0d1aa36ac4eceb53d1d5d2cf1caf9)
  - test/test_sort_and_select.py::TestSortAndSelectCPU::test_sort_discontiguous_slow_cpu_float32 [GH job link](https://github.com/pytorch/pytorch/actions/runs/10994904767/job/30525578456) [HUD commit link](https://hud.pytorch.org/pytorch/pytorch/commit/239a9ad65eebf93dcf9bb108a5129d4160b12c86) ([comment](https://github.com/pytorch/pytorch/pull/127936#issuecomment-2368522316))
- [Revert "Allow fx graph caching higher order operators (opt-in) (#135877)"](https://github.com/pytorch/pytorch/commit/e9bfbf78d5d89df1ec59cb82d7f78b85f9014a98)
  - seems to have introduced regressions on rocm signals ([comment](https://github.com/pytorch/pytorch/pull/135877#issuecomment-2367616653))

### Weird (2)

- [Revert "[Flex Attention] fix block size order (#136657)"](https://github.com/pytorch/pytorch/commit/b408591b5380d6b856e0a4ce32cf386c56660c54)
  - Sorry for reverting your change but some test_flex_attention is failing in trunk after this change https://hud.pytorch.org/pytorch/pytorch/commit/529b6ab0bb9f8800ed795ec8e4fa1f0e8042bb0a ([comment](https://github.com/pytorch/pytorch/pull/136657#issuecomment-2375824802))
- [Revert "[ONNX] Remove the operators test (#136335)"](https://github.com/pytorch/pytorch/commit/5171b0e3c6ee43c27c0360672cb8b642d52d0410)
  - I'll reland this, bear with me ([comment](https://github.com/pytorch/pytorch/pull/136335#issuecomment-2374183435))
