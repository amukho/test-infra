# Week of 2025-02-17 to 2025-02-24 (14)

### GHFirst (8)

- [Revert "[ROCm] OCP FP8 Support for new GPUs (#146632)"](https://github.com/pytorch/pytorch/commit/3e2d9d079e49d3dda6069cddd8dcb0d2e5cb5b48)
  - Breaking internal builds, I'll find someone to help merge this PR back to main ([comment](https://github.com/pytorch/pytorch/pull/146632#issuecomment-2676823614))
- [Revert "[cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) (#141178)"](https://github.com/pytorch/pytorch/commit/fa8e3a28a7bab47742d7f791a24eeb87b04469ca)
  - Broke internal arvr signals, see D69971019. @jbschlosser please help the author get this PR merged ([comment](https://github.com/pytorch/pytorch/pull/141178#issuecomment-2676317470))
- [Revert "[ROCm] Implemented dropout usage for RNN with MIOpen backend (#144572)"](https://github.com/pytorch/pytorch/commit/bea72180ed75f522ce4fe5e723bc2112e0874732)
  - Broke internal signals, D69994027, I'll find someone to help get this change merged ([comment](https://github.com/pytorch/pytorch/pull/144572#issuecomment-2676314308))
- [Revert "Delete Mixed MM Special Casing (#147151)"](https://github.com/pytorch/pytorch/commit/3409cbd1770af671ecde6bbd167d069696cfd83f)
  - Broke a few internal signals, see comments on D69994157 ([comment](https://github.com/pytorch/pytorch/pull/147151#issuecomment-2676312215))
- [Revert "[Inductor][Triton] Rework casting logic to avoid illegal bitcast (#147395)"](https://github.com/pytorch/pytorch/commit/05e6f15966d9ce865df8088fba510e8b656276d7)
  - Breaking internal builds, see D69890757 - servicelab_benchmark_pyper_local_runner, @eellison please help the author get this change landed ([comment](https://github.com/pytorch/pytorch/pull/147395#issuecomment-2675521966))
- [Revert "Add torch._scaled_mm for CPU (#139975)"](https://github.com/pytorch/pytorch/commit/babb2dc2afd8f9ace955df3e8773664ee6e438a7)
  - failing inductor mkldnn_pattern_matcher_cpu tests ([comment](https://github.com/pytorch/pytorch/pull/139975#issuecomment-2667186865))
- [Revert "Fix non-bitwise type annotations for Tensor operators (see #145838) (#146845)"](https://github.com/pytorch/pytorch/commit/302f56a1f2d508b5b9eaa01738535e4bdd3386e1)
  - Seems to break a few code dependencies in multiple places ([comment](https://github.com/pytorch/pytorch/pull/146845#issuecomment-2666656834))
- [Revert "Add torch._scaled_mm for CPU (#139975)"](https://github.com/pytorch/pytorch/commit/49e8f9c965118c5b4b0c4727643351e3ca2b7691)
  - third time is the charm ([comment](https://github.com/pytorch/pytorch/pull/139975#issuecomment-2664622598))

### Ignored Signal (1)

- [Revert "Build a storage reader/writer to write checkpoints in HF format (#146352)"](https://github.com/pytorch/pytorch/commit/3395da7f7cb132b082157a5ba19c83fa27612c3d)
  - Author ignored linting errors ([comment](https://github.com/pytorch/pytorch/pull/146352#issuecomment-2673789271))

### Not through pytorchbot (2)

- [Revert "Add cifllow/riscv64 label"](https://github.com/pytorch/pytorch/commit/ead970c8d035690c180641909b75da13fa16c76e)
- [Revert "[ROCm] ROCm-specific gemm tuning parameters" (#147388)](https://github.com/pytorch/pytorch/commit/465930ee8152fb17c064f0837246d1a25b325d78)

### No Signal (2)

- [Revert "[trymerge] Post initial starting merge comment on stacked PRs (#147028)"](https://github.com/pytorch/pytorch/commit/ef6b16ea9d200cba87c8730c99264d6d477ab0ec)
  - I think this broke merge for non ghstack prs ([comment](https://github.com/pytorch/pytorch/pull/147028#issuecomment-2675532017))
- [Revert "Nccl update to 2.25.1 for cuda 12.4-12.8  (#146073)"](https://github.com/pytorch/pytorch/commit/7622e29a3748496aaddef9525c2712cfe87f97b1)
  - breaks Locally building benchmarks ([comment](https://github.com/pytorch/pytorch/pull/146073#issuecomment-2667054179))

### Weird (1)

- [Revert "Increase memory for linux binary builds  (#147542)"](https://github.com/pytorch/pytorch/commit/e5da9df421f98308aa2af604e94b2d111372916f)
  - seems that it is best to use another machine type ([comment](https://github.com/pytorch/pytorch/pull/147542#issuecomment-2673765724))
