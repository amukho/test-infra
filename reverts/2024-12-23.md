# Week of 2024-12-23 to 2024-12-30 (16)

### GHFirst (5)

- [Revert "fix randint distribution for large max (#143787)"](https://github.com/pytorch/pytorch/commit/35714767395a509625fbca128d4a9293a7812410)
  - failing internal tests, to be fixed first ([comment](https://github.com/pytorch/pytorch/pull/143787#issuecomment-2563493323))
- [Revert "Use random64 in Fischer-Yates algorithm for large N (#143682)"](https://github.com/pytorch/pytorch/commit/f6801ba4b301e20fa84b7a350bc29cb40134934c)
  - failing Meta internal tests that need to be updated ([comment](https://github.com/pytorch/pytorch/pull/143682#issuecomment-2563487675))
- [Revert "[dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys (#143722)"](https://github.com/pytorch/pytorch/commit/26364428f5bf09cb2639eed5f62654af2ea9723e)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/143722#issuecomment-2563127017))
- [Revert "[dynamo] Remove dead code after introducing UserDefinedDictVariable (#143699)"](https://github.com/pytorch/pytorch/commit/ee25daef5a49d17c51b56595f6d715ad2ee8d154)
  - failing internal tests ([comment](https://github.com/pytorch/pytorch/pull/143722#issuecomment-2563127017))
- [Revert "[BE][Easy] use `pathlib.Path` instead of `dirname` / `".."` / `pardir` (#129374)"](https://github.com/pytorch/pytorch/commit/475656fd9c3871302e164bcd45415f18d68f637b)
  - failing internal ROCM builds with error: ModuleNotFoundError: No module named hipify ([comment](https://github.com/pytorch/pytorch/pull/129374#issuecomment-2562973920))

### Ignored Signal (3)

- [Revert "Add torch._scaled_mm for CPU (#139975)"](https://github.com/pytorch/pytorch/commit/45a709d9ec5974db79d0cd14127c5a6349c9fc99)
  - It broke the same test, but on ROCM this time, though it was classified as flaky for some reason, see https://hud.pytorch.org/hud/pytorch/pytorch/d8c3900d80c4f170759775fd2500d08f3ee83166/1?per_page=50&name_filter=rocm%20%2F%20linux-focal-rocm6.2-py3.10%20%2F%20test&mergeLF=true ([comment](https://github.com/pytorch/pytorch/pull/139975#issuecomment-2564378146))
- [Revert "Add AOT inductor support for _scaled_mm for CPU (#141961)"](https://github.com/pytorch/pytorch/commit/8cccc46e334e0d72e1e1d6923c290a241ec360fd)
  - It broke the same test, but on ROCM this time, though it was classified as flaky for some reason, see https://hud.pytorch.org/hud/pytorch/pytorch/d8c3900d80c4f170759775fd2500d08f3ee83166/1?per_page=50&name_filter=rocm%20%2F%20linux-focal-rocm6.2-py3.10%20%2F%20test&mergeLF=true ([comment](https://github.com/pytorch/pytorch/pull/139975#issuecomment-2564378146))
- [Revert "[reland][AMD] Turn on TF32 for aten::mm (#143549)"](https://github.com/pytorch/pytorch/commit/448c16ac87f73dfce40778c9ca1373aea369fd67)
  - It breaks ROCM testing, see https://hud.pytorch.org/hud/pytorch/pytorch/06b4b96b34e93cc85f48b083cf04e7598fd68e20/1?per_page=50&name_filter=rocm%20%2F%20linux-focal-rocm6.2 ([comment](https://github.com/pytorch/pytorch/pull/143549#issuecomment-2559016960))

### No Signal (8)

- [Revert "remove allow-untyped-defs from torch/ao/__init__.py (#143604)"](https://github.com/pytorch/pytorch/commit/b5042cfa58ba447d653e9463de840f7666328596)
  - failing typing checks in torchao ([comment](https://github.com/pytorch/pytorch/pull/143604#issuecomment-2564043233))
- [Revert "Add torch._scaled_mm for CPU (#139975)"](https://github.com/pytorch/pytorch/commit/fca457b5db78eb6ba13159d4a135c7ee43bce2bb)
  - Sorry for reverting your change but it is failing some tests in trunk ([comment](https://github.com/pytorch/pytorch/pull/139975#issuecomment-2563331259))
- [Revert "Use absolute path `path.resolve()` -> `path.absolute()` (#129409)"](https://github.com/pytorch/pytorch/commit/cc4e70b7c3fbbe945ace3ecad88b72bae6e3e5ed)
  - need to revert to as dependency of https://github.com/pytorch/pytorch/pull/129374 ([comment](https://github.com/pytorch/pytorch/pull/129409#issuecomment-2562969825))
- [Revert "Enable more C++ warnings (#143355)"](https://github.com/pytorch/pytorch/commit/9255ffc84145bdf6b849b1a297bdba25fe01d04e)
  - It fails internal build system as it kind of breaks separation between native and native/cpu ([comment](https://github.com/pytorch/pytorch/pull/143355#issuecomment-2562961546))
- [Revert "[Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. (#143266)"](https://github.com/pytorch/pytorch/commit/844e6108f6b972118b391005f875057ced6b96bc)
  - Sorry for reverting your change but it is failing some tests in trunk ([comment](https://github.com/pytorch/pytorch/pull/143266#issuecomment-2561303786))
- [Revert "Add a warning when a tensor with requires_grad=True is converted to a scalar (#143261)"](https://github.com/pytorch/pytorch/commit/49fdc52fd2b626df5304e3563c205f3e42b9a732)
  - Sorry for reverting your change but it is failing lint, plz help fix and reland this ([comment](https://github.com/pytorch/pytorch/pull/143261#issuecomment-2560583332))
- [Revert "Exclude py 31.3t triton package from PyTorch 3.13t wheel (#143218)"](https://github.com/pytorch/pytorch/commit/0ebc6388cf13744d185c24badbf2cddb69e643f7)
  - this constrain is ignored see https://github.com/pytorch/pytorch/issues/143654 ([comment](https://github.com/pytorch/pytorch/pull/143218#issuecomment-2560208992))
- [Revert "Add FP8 support for eye (#139974)"](https://github.com/pytorch/pytorch/commit/1519a9e30b65ed8c20341c07aa925007f527eb8f)
  - Sorry for reverting your change but this seems to fail some slow tests ([comment](https://github.com/pytorch/pytorch/pull/139974#issuecomment-2560046399))
